{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook centers on predicting the maximum depth of anomalies in In-Line Inspection (ILI) data. Utilizing various machine learning techniques, the goal is to fill in missing values and forecast the future growth of anomalies. Accurately estimating anomaly depth is critical for assessing pipeline strength and ensuring safety. The process involves data exploration, cleaning, feature engineering, anomaly mapping, and advanced modeling. These steps offer valuable insights for managing pipeline integrity, enabling proactive maintenance and risk mitigation.\n",
    "\n",
    "The ILI data for this study is publicly available from the [Mendeley Data repository](https://data.mendeley.com/datasets/c2h2jf5c54/1). The dataset, titled \"Dataset for: Cross-country Pipeline Inspection Data Analysis and Testing of Probabilistic Degradation Models\", was published on October 4, 2021, by Rioshar Yarveisy, Faisal Khan, and Rouzbeh Abbassi from Memorial University of Newfoundland and Macquarie University. The dataset includes four consecutive ILI data sets, which lack certain details such as coordinates, likely due to anonymization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline integrity management is crucial in ensuring the safety and reliability of gas and oil transportation. In-line inspection (ILI) tools are extensively used to detect and measure anomalies in pipelines. Accurately predicting the maximum depth of these anomalies is essential for proactive maintenance and risk mitigation. This notebook demonstrates a comprehensive workflow, from data loading and cleaning to advanced machine learning modeling, aimed at predicting anomaly depths effectively. Key steps in the process include:\n",
    "\n",
    "**Data Exploration and Cleaning**: This involves exploratory data analysis (EDA) to understand the data distribution and identify patterns, handling duplicate records, and managing missing values.\n",
    "\n",
    "**Feature Engineering**: We compute new features such as aspect ratio and area of anomalies, estimate the maximum depth using domain-specific calculations, and create cyclic features from angular measurements.\n",
    "\n",
    "**Anomaly Mapping**: We match anomalies across different inspection years to track their growth and changes over time. This involves sophisticated matching algorithms to identify corresponding anomalies based on relative distances and orientations.\n",
    "\n",
    "**Modeling**: We employ machine learning models, particularly the Hist Gradient Boosting Regressor, to predict the maximum depth of anomalies. This includes data preparation, model training, hyperparameter tuning, and evaluation.\n",
    "\n",
    "**Prediction and Validation**: The predicted values are validated against actual measurements to ensure accuracy. We also compare the machine learning predictions with domain-specific estimates to highlight the added value of advanced modeling techniques.\n",
    "\n",
    "The ILI data for this study is publicly available from the [Mendeley Data repository](https://data.mendeley.com/datasets/c2h2jf5c54/1). The dataset, titled \"Dataset for: Cross-country Pipeline Inspection Data Analysis and Testing of Probabilistic Degradation Models,\" was published on October 4, 2021, by Rioshar Yarveisy, Faisal Khan, and Rouzbeh Abbassi from Memorial University of Newfoundland and Macquarie University. The dataset includes four consecutive ILI data sets, which lack certain details such as coordinates, likely due to anonymization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from src import tools\n",
    "importlib.reload(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory to save and read the data\n",
    "save_path = \"Dataset/processed_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the ILI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnomaliesProc  = pd.read_csv(os.path.join(save_path, f'external_anomoly_processed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import rename_anomaly_columns\n",
    "\n",
    "# Rename the columns\n",
    "AnomaliesProc = rename_anomaly_columns(AnomaliesProc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Depth (mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the EDA object\n",
    "eda = EDA(AnomaliesProc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the max depth\n",
    "eda.plot_histogram_max_depth('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(AnomaliesProc['MaxDepth_mm'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles and IQR\n",
    "eda.calculate_percentiles_and_iqr('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "eda.plot_boxplot_max_depth('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.plot_correlation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = AnomaliesProc.duplicated(keep=False)\n",
    "\n",
    "# Display the duplicate rows\n",
    "duplicate_rows = AnomaliesProc[duplicates]\n",
    "\n",
    "# Print the duplicate rows\n",
    "print(\"Duplicate rows in the dataframe:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import MissingValuesAnalyzer\n",
    "\n",
    "# Create the MissingValuesAnalyzer object\n",
    "MissingValuesAnalyzer = MissingValuesAnalyzer(AnomaliesProc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Features w/ Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Point Distance                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation only if the 'EndPointDistance_m' column has NaN values\n",
    "AnomaliesProc['EndPointDistance_m'] = np.where(\n",
    "    AnomaliesProc['EndPointDistance_m'].isna(),\n",
    "    AnomaliesProc['StartPointDistance_m'] + AnomaliesProc['FeatureLength_mm'] / 1000,\n",
    "    AnomaliesProc['EndPointDistance_m']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the remaining columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seam Orientation             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Joints with Inconsistent Seam Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle joints with inconsistent seam orientation\n",
    "MissingValuesAnalyzer.check_inconsistent_seam_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle joints with inconsistent seam orientation\n",
    "MissingValuesAnalyzer.handle_inconsistent_seam_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and report the inconsistent joints\n",
    "MissingValuesAnalyzer.find_and_report_inconsistent_joints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Joints with missing Values   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation using Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing seam orientation values with the average. Since each joints has a unique seam orientation, the average is the same as the original value.\n",
    "AnomaliesProc = MissingValuesAnalyzer.fill_missing_seam_orientation_w_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values. The remaining missing values occurs in joints with no seam orientation across all inspection years. For those, we can use fill forward from the previous joint.\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation using Fill Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing seam orientation values with the previous value\n",
    "AnomaliesProc =  MissingValuesAnalyzer.fill_missing_seam_orientation_w_ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting Anomaly Depth: A Machine Learning Approach\n",
    "This exercise aims to predict the maximum depth of anomalies for educational purposes. The applications of this prediction include filling in missing data and forecasting the future growth of anomalies, particularly the maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation (Optional)\n",
    "Random removal of MaxDepth data to then predict them using ML for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly remove 5% of the values from MaxDepth_mm column\n",
    "# mask = np.random.rand(len(AnomaliesProc)) < 0.01  # Adjusted to 1%\n",
    "# AnomaliesProc.loc[mask, 'MaxDepth_mm'] = np.nan\n",
    "\n",
    "# Verify the number of missing values\n",
    "# print(f\"Number of missing values: {AnomaliesProc['MaxDepth_mm'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip the model selection and hyperparameter tuning step and use Hist Gradient Boosting Regressor with default hyperparameter to predict the records with missing max_depth_mm values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.callbacks import DeltaYStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame\n",
    "AnomaliesProc_ML_Ready = AnomaliesProc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import FeatureEngineering\n",
    "\n",
    "# Create an instance of the class with your dataframe\n",
    "feature_engineering = FeatureEngineering(AnomaliesProc_ML_Ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Ratio and Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the aspect ratio\n",
    "AnomaliesProc_ML_Ready = feature_engineering.compute_aspect_ratio()\n",
    "\n",
    "# Calculate the feature area\n",
    "AnomaliesProc_ML_Ready = feature_engineering.calculate_feature_area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial to Cyclic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the angular features\n",
    "angle_columns = ['SeamOrientation_deg', 'StartPointOrientation_deg', 'EndPointOrientation_deg', 'SignificantPointOrientation_deg']\n",
    "AnomaliesProc_ML_Ready = feature_engineering.add_angular_features(angle_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import Anomaly_mapping, plot_anomalies_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Mapping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "increment_size = 1000\n",
    "relative_distance_threshold = 0.1  # meters\n",
    "orientation_threshold = 10  # degrees\n",
    "\n",
    "# Create an instance of Anomaly_mapping using AnomaliesProc_ML_Ready\n",
    "anomaly_mapper = Anomaly_mapping(AnomaliesProc_ML_Ready, relative_distance_threshold, orientation_threshold)\n",
    "\n",
    "# Call the process_in_increments method and store the result in Anomaly_mapped_df\n",
    "Anomaly_mapped_df = anomaly_mapper.process_in_increments(save_path, increment_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the anomalies by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "Plot_Anomaly_mapped_df_file_path = (os.path.join(save_path, f'Plot_Mapped_Anomalies.csv'))\n",
    "\n",
    "Anomaly_mapped_df[[\n",
    "    'GirthWeldNumber',\n",
    "    'InspectionYear',\n",
    "    'RelativeDistance_m',\n",
    "    'Tag',\n",
    "    'SignificantPointOrientation_deg'\n",
    "]].to_csv(Plot_Anomaly_mapped_df_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the anomalies by year\n",
    "plot_anomalies_by_year(Anomaly_mapped_df, 14, figsize=(15, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag Erroneous Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import ErroneousAnomalyProcessor\n",
    "\n",
    "# Load the AnomaliesProc_Mapped_All_GirthWelds.csv file\n",
    "Anomaly_mapped_df = pd.read_csv(os.path.join(save_path, f'AnomaliesProc_Mapped_All_GirthWelds.csv'))\n",
    "\n",
    "# Detect errors in mapped anomalies\n",
    "anomaly_processor = ErroneousAnomalyProcessor(Anomaly_mapped_df)\n",
    "\n",
    "# Apply the detect_errors method\n",
    "Anomaly_mapped_df['ErrorClassification'] = Anomaly_mapped_df.apply(\n",
    "    anomaly_processor.detect_errors, axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "Anomaly_mapped_validated_df_file_path = (os.path.join(save_path, f'AnomaliesProc_Mapped_All_GirthWelds_Validated.csv'))\n",
    "Anomaly_mapped_df.to_csv(Anomaly_mapped_validated_df_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "Anomaly_mapped_validated_df_file_path = '../Dataset/processed_data/AnomaliesProc_Mapped_All_GirthWelds_Validated.csv'\n",
    "\n",
    "Anomaly_mapped_df = pd.read_csv(Anomaly_mapped_validated_df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print error statistics\n",
    "ErroneousAnomalyProcessor.print_error_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the 'Okay' records\n",
    "Filtered_Anomaly_mapped_df = Anomaly_mapped_df[Anomaly_mapped_df.ErrorClassification == 'Okay']\n",
    "\n",
    "# Filter the DataFrame to include only the 'old' and 'new' records\n",
    "Old_Filtered_Anomaly_mapped_df = Filtered_Anomaly_mapped_df[Filtered_Anomaly_mapped_df.Tag == 'old']\n",
    "New_Filtered_Anomaly_mapped_df = Filtered_Anomaly_mapped_df[Filtered_Anomaly_mapped_df.Tag == 'new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "Old_Filtered_Anomaly_mapped_df_file_path = (os.path.join(save_path, f'Old_Filtered_Anomaly_mapped_df.csv'))\n",
    "Old_Filtered_Anomaly_mapped_df.to_csv(Old_Filtered_Anomaly_mapped_df_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include Second Prior Inspection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import add_dprev_features\n",
    "\n",
    "# Add the secont previous inspection year features to the DataFrame\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df = add_dprev_features(Old_Filtered_Anomaly_mapped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to store the updated DataFrame\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df_file_path = (os.path.join(save_path, f'DPrev_Old_Filtered_Anomaly_mapped_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df.to_csv(DPrev_Old_Filtered_Anomaly_mapped_df_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the updated DataFrame from the CSV file\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df = pd.read_csv(DPrev_Old_Filtered_Anomaly_mapped_df_file_path)\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import HandlingOutlier\n",
    "\n",
    "# Example usage\n",
    "handling_outlier_columns = ['MaxDepth_mm', 'FeatureWidth_mm', 'FeatureLength_mm', 'InspectionYear']\n",
    "\n",
    "# Create an instance of the HandlingOutlier class\n",
    "outlier_handler = HandlingOutlier(DPrev_Old_Filtered_Anomaly_mapped_df)\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df = outlier_handler.remove_outliers_zscore(handling_outlier_columns)\n",
    "\n",
    "# Remove outliers using Isolation Forest method\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df = outlier_handler.remove_outliers_isolation_forest(handling_outlier_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm'].value_counts().sort_index()\n",
    "sns.histplot(DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm'], bins=100)\n",
    "plt.title('Hist Plot of MaxDepth_mm')\n",
    "plt.xlabel('MaxDepth_mm')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regularization algorithm is used for to identify feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = DPrev_Old_Filtered_Anomaly_mapped_df.drop(\n",
    "    columns=[\n",
    "        'MaxDepth_mm',\n",
    "        'Tag',\n",
    "        'ErrorClassification',\n",
    "        'Prev_MaxDepth_mm',\n",
    "        'DepthChange',\n",
    "        'LengthChange',\n",
    "        'WidthChange',\n",
    "        'DistanceDiff',\n",
    "        'OrientationDiff'\n",
    "    ]\n",
    ")\n",
    "\n",
    "target = DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import FeatureImportance\n",
    "\n",
    "feature_importance = FeatureImportance(features, target)\n",
    "\n",
    "# Perform the steps\n",
    "feature_importance.standardize_features()\n",
    "feature_importance.split_data()\n",
    "feature_importance.perform_grid_search()\n",
    "feature_importance.fit_best_lasso()\n",
    "feature_importance.calculate_coefficients()\n",
    "\n",
    "# Plot the coefficients\n",
    "feature_importance.plot_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to remove outliers only from the target variable\n",
    "target = DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm']\n",
    "\n",
    "# Keep all features, including rows where target is an outlier\n",
    "features = DPrev_Old_Filtered_Anomaly_mapped_df[[\n",
    "        'GirthWeldNumber',\n",
    "        'InspectionYear',\n",
    "        'RelativeDistance_m',\n",
    "        'FeatureLength_mm',\n",
    "        'FeatureWidth_mm',\n",
    "        'SignificantPointOrientation_deg',\n",
    "        'Prev_InspectionYear',\n",
    "        'Prev_RelativeDistance_m',\n",
    "        'Prev_FeatureLength_mm',\n",
    "        'Prev_FeatureWidth_mm',\n",
    "        'Prev_MaxDepth_mm',\n",
    "        'Prev_SignificantPointOrientation_deg',\n",
    "        'DPrev_RelativeDistance_m',\n",
    "        'DPrev_FeatureLength_mm',\n",
    "        'DPrev_FeatureWidth_mm',\n",
    "        'DPrev_MaxDepth_mm',\n",
    "        'DPrev_SignificantPointOrientation_deg',\n",
    "        'AspectRatio',\n",
    "        'FeatureArea_mm2',\n",
    "        'estimated_max_depth_mm',\n",
    "        'SignificantPointOrientation_deg_sin',\n",
    "        'SignificantPointOrientation_deg_cos',\n",
    "        'LengthChange',\n",
    "        'WidthChange',\n",
    "        'DistanceDiff',\n",
    "        'OrientationDiff'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import TrainingPipeline\n",
    "\n",
    "# Create an instance of the TrainingPipeline class\n",
    "ML_pipeline = TrainingPipeline(features, target)\n",
    "print(\"Pipeline instance created\")\n",
    "\n",
    "# Scale the features\n",
    "ML_pipeline.scale_features()\n",
    "print(\"Features scaled\")\n",
    "\n",
    "# Split the data\n",
    "ML_pipeline.split_data(handle_imbalance=True)\n",
    "print(\"Data split\")\n",
    "\n",
    "# Perform hyperparameter tuning and return the best parameters\n",
    "best_params = ML_pipeline.hyperparameter_tuning()\n",
    "print(\"Best parameters found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fit the model using the best hyper parameters\n",
    "ML_pipeline.fit_model()\n",
    "print(\"Model fitted\")\n",
    "\n",
    "# Evaluate the model and print the metrics\n",
    "evaluation_metrics = ML_pipeline.evaluate_model()\n",
    "for metrics,performance in evaluation_metrics.items():\n",
    "    print(f\"{metrics}: {performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results and evaluating prediction accuracy\n",
    "results = ML_pipeline.plot_prediction_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_violin(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_scatter(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values\n",
    "predicted_records = ML_pipeline.fill_missing_values(dataframe=AnomaliesProc_ML_Ready, feature_columns=features.columns, target_column='MaxDepth_mm')\n",
    "print(predicted_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Anomaly Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPrev_Old_Filtered_Anomaly_mapped_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPrev_Old_Filtered_Anomaly_mapped_df['Estimated_FeatureLength_mm'] = 2 * DPrev_Old_Filtered_Anomaly_mapped_df['Prev_FeatureLength_mm'] \\\n",
    "                                                                       - DPrev_Old_Filtered_Anomaly_mapped_df['DPrev_FeatureLength_mm']\n",
    "\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df['Estimated_FeatureWidth_mm'] = 2 * DPrev_Old_Filtered_Anomaly_mapped_df['Prev_FeatureWidth_mm'] \\\n",
    "                                                                        - DPrev_Old_Filtered_Anomaly_mapped_df['DPrev_FeatureWidth_mm']\n",
    "\n",
    "DPrev_Old_Filtered_Anomaly_mapped_df['Powered_Prev_MaxDepth_mm'] = DPrev_Old_Filtered_Anomaly_mapped_df['Prev_MaxDepth_mm'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to remove outliers only from the target variable\n",
    "target = DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm']\n",
    "\n",
    "# Define the list of features\n",
    "feature_columns = [\n",
    "        'GirthWeldNumber',\n",
    "        'InspectionYear',\n",
    "        'RelativeDistance_m',\n",
    "        'Estimated_FeatureLength_mm',\n",
    "        'Estimated_FeatureWidth_mm',\n",
    "        'SignificantPointOrientation_deg',\n",
    "        'Prev_InspectionYear',\n",
    "        'Prev_RelativeDistance_m',\n",
    "        'Prev_FeatureLength_mm',\n",
    "        'Prev_FeatureWidth_mm',\n",
    "        'Prev_MaxDepth_mm',\n",
    "        'Powered_Prev_MaxDepth_mm',\n",
    "        'Prev_SignificantPointOrientation_deg',\n",
    "        'DPrev_RelativeDistance_m',\n",
    "        'DPrev_FeatureLength_mm',\n",
    "        'DPrev_FeatureWidth_mm',\n",
    "        'DPrev_MaxDepth_mm',\n",
    "        'DPrev_SignificantPointOrientation_deg',\n",
    "        'SignificantPointOrientation_deg_sin',\n",
    "        'SignificantPointOrientation_deg_cos'\n",
    "    ]\n",
    "\n",
    "# Keep all features, including rows where target is an outlier\n",
    "features = DPrev_Old_Filtered_Anomaly_mapped_df[feature_columns]\n",
    "\n",
    "# Ensure features and target have the same index\n",
    "features = features.loc[target.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import TrainingPipeline\n",
    "\n",
    "# Create an instance of the TrainingPipeline class\n",
    "ML_pipeline = TrainingPipeline(features, target)\n",
    "print(\"Pipeline instance created\")\n",
    "\n",
    "# Scale the features\n",
    "ML_pipeline.scale_features()\n",
    "print(\"Features scaled\")\n",
    "\n",
    "# Split the data\n",
    "ML_pipeline.split_data(handle_imbalance=True)\n",
    "print(\"Data split\")\n",
    "\n",
    "# Perform hyperparameter tuning and return the best parameters\n",
    "best_params = ML_pipeline.hyperparameter_tuning()\n",
    "print(\"Best parameters found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fit the model using the best hyper parameters\n",
    "ML_pipeline.fit_model()\n",
    "print(\"Model fitted\")\n",
    "\n",
    "# Evaluate the model and print the metrics\n",
    "evaluation_metrics = ML_pipeline.evaluate_model()\n",
    "for metrics,performance in evaluation_metrics.items():\n",
    "    print(f\"{metrics}: {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting results and evaluating prediction accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ML_pipeline.plot_prediction_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_violin(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_scatter(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
