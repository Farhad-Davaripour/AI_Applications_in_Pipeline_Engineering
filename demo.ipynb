{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI in Pipeline Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook centers on predicting the maximum depth of anomalies in In-Line Inspection (ILI) data. Utilizing various machine learning techniques, the goal is to fill in missing values and forecast the future growth of anomalies. Accurately estimating anomaly depth is critical for assessing pipeline strength and ensuring safety. The process involves data exploration, cleaning, feature engineering, anomaly mapping, and advanced modeling. These steps offer valuable insights for managing pipeline integrity, enabling proactive maintenance and risk mitigation.\n",
    "\n",
    "The ILI data for this study is publicly available from the [Mendeley Data repository](https://data.mendeley.com/datasets/c2h2jf5c54/1). The dataset, titled \"Dataset for: Cross-country Pipeline Inspection Data Analysis and Testing of Probabilistic Degradation Models\", was published on October 4, 2021, by Rioshar Yarveisy, Faisal Khan, and Rouzbeh Abbassi from Memorial University of Newfoundland and Macquarie University. The dataset includes four consecutive ILI data sets, which lack certain details such as coordinates, likely due to anonymization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline integrity management is crucial in ensuring the safety and reliability of gas and oil transportation. In-line inspection (ILI) tools are extensively used to detect and measure anomalies in pipelines. Accurately predicting the maximum depth of these anomalies is essential for proactive maintenance and risk mitigation. This notebook demonstrates a comprehensive workflow, from data loading and cleaning to advanced machine learning modeling, aimed at predicting anomaly depths effectively. Key steps in the process include:\n",
    "\n",
    "**Data Exploration and Cleaning**: This involves exploratory data analysis (EDA) to understand the data distribution and identify patterns, handling duplicate records, and managing missing values.\n",
    "\n",
    "**Feature Engineering**: We compute new features such as aspect ratio and area of anomalies, estimate the maximum depth using domain-specific calculations, and create cyclic features from angular measurements.\n",
    "\n",
    "**Anomaly Mapping**: We match anomalies across different inspection years to track their growth and changes over time. This involves sophisticated matching algorithms to identify corresponding anomalies based on relative distances and orientations.\n",
    "\n",
    "**Modeling**: We employ machine learning models, particularly the Hist Gradient Boosting Regressor, to predict the maximum depth of anomalies. This includes data preparation, model training, hyperparameter tuning, and evaluation.\n",
    "\n",
    "**Prediction and Validation**: The predicted values are validated against actual measurements to ensure accuracy. We also compare the machine learning predictions with domain-specific estimates to highlight the added value of advanced modeling techniques.\n",
    "\n",
    "The ILI data for this study is publicly available from the [Mendeley Data repository](https://data.mendeley.com/datasets/c2h2jf5c54/1). The dataset, titled \"Dataset for: Cross-country Pipeline Inspection Data Analysis and Testing of Probabilistic Degradation Models,\" was published on October 4, 2021, by Rioshar Yarveisy, Faisal Khan, and Rouzbeh Abbassi from Memorial University of Newfoundland and Macquarie University. The dataset includes four consecutive ILI data sets, which lack certain details such as coordinates, likely due to anonymization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.tools' from 'c:\\\\Users\\\\Farhad.Davaripour\\\\Repositories\\\\AI_Applications_in_Pipeline_Engineering\\\\src\\\\tools.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from src import tools\n",
    "importlib.reload(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(override=True)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_me(user_query):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant that answers to the user query.\"},\n",
    "                {\"role\": \"user\", \"content\": user_query}\n",
    "                ]\n",
    "    completion = client.chat.completions.create(\n",
    "                model='gpt-4o-mini', \n",
    "                temperature=0,\n",
    "                messages=messages)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "user_query = 'Provide concise description of International Pipeline Conference (IPC) conference in Calgary? keep it in under 20 words.'\n",
    "print(ask_me(user_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the ILI Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data locally - Set the directory to save and read the data\n",
    "# save_path = \"Dataset/processed_data/\"\n",
    "# Anomalies_df  = pd.read_parquet(os.path.join(save_path, f'Anomoly_processed.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the raw Parquet file from GitHub\n",
    "url = 'https://github.com/Farhad-Davaripour/AI_Applications_in_Pipeline_Engineering/raw/main/Dataset/processed_data/Anomoly_processed.parquet'\n",
    "\n",
    "# Use pandas with fsspec to read the Parquet file directly\n",
    "Anomalies_df = pd.read_parquet(url, engine='pyarrow', storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import rename_anomaly_columns\n",
    "\n",
    "# Rename the columns to make them more readable\n",
    "Anomalies_df = rename_anomaly_columns(Anomalies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for col in Anomalies_df.columns:\n",
    "    print(f\"column #{i+1}: {col}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix Data Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomalies_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomalies_df['InspectionYear'] = Anomalies_df['InspectionYear'].astype(int)\n",
    "Anomalies_df['GirthWeldNumber'] = Anomalies_df['GirthWeldNumber'].astype(int)\n",
    "Anomalies_df['WallThickness_mm'] = Anomalies_df['WallThickness_mm'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomalies_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max Depth (mm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the EDA object\n",
    "eda = EDA(Anomalies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the max depth\n",
    "eda.plot_histogram_max_depth('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "Anomalies_df['MaxDepth_mm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles and IQR\n",
    "eda.calculate_percentiles_and_iqr('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "eda.plot_boxplot_max_depth('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.plot_correlation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = Anomalies_df.duplicated(keep=False)\n",
    "\n",
    "# Display the duplicate rows\n",
    "duplicate_rows = Anomalies_df[duplicates]\n",
    "\n",
    "# Print the duplicate rows\n",
    "print(\"Duplicate rows in the dataframe:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import MissingValuesAnalyzer\n",
    "\n",
    "# Create the MissingValuesAnalyzer object\n",
    "MissingValuesAnalyzer = MissingValuesAnalyzer(Anomalies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Features w/ Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 End Point Distance                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation only if the 'EndPointDistance_m' column has NaN values\n",
    "Anomalies_df['EndPointDistance_m'] = np.where(\n",
    "    Anomalies_df['EndPointDistance_m'].isna(),\n",
    "    Anomalies_df['StartPointDistance_m'] + Anomalies_df['FeatureLength_mm'] / 1000,\n",
    "    Anomalies_df['EndPointDistance_m']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the remaining columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Seam Orientation             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1 Handle Joints with Inconsistent Seam Orientation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle joints with inconsistent seam orientation and print only the last joint\n",
    "MissingValuesAnalyzer.check_inconsistent_seam_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle joints with inconsistent seam orientation\n",
    "Anomalies_df = MissingValuesAnalyzer.handle_inconsistent_seam_orientation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and report the inconsistent joints\n",
    "MissingValuesAnalyzer.find_and_report_inconsistent_joints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2 Handle Joints with missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutation using Mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing seam orientation values with the average. Since each joints has a unique seam orientation, the average is the same as the original value.\n",
    "Anomalies_df, filled = MissingValuesAnalyzer.fill_missing_seam_orientation_w_average()\n",
    "print(f\"number of filled values: {filled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values. The remaining missing values occurs in joints with no seam orientation across all inspection years. For those, we can use fill forward from the previous joint.\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutation using Fill Forward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing seam orientation values with the previous value\n",
    "AnomaliesProc =  MissingValuesAnalyzer.fill_missing_seam_orientation_w_ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values\n",
    "MissingValuesAnalyzer.find_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import HandlingOutlier\n",
    "\n",
    "# Example usage\n",
    "handling_outlier_columns = ['MaxDepth_mm', 'FeatureWidth_mm', 'FeatureLength_mm', 'InspectionYear']\n",
    "\n",
    "# Create an instance of the HandlingOutlier class\n",
    "outlier_handler = HandlingOutlier(AnomaliesProc)\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "Anomalies_OutliersAdjusted_df = outlier_handler.remove_outliers_zscore(handling_outlier_columns)\n",
    "\n",
    "# Remove outliers using Isolation Forest method\n",
    "Anomalies_OutliersAdjusted_df = outlier_handler.remove_outliers_isolation_forest(handling_outlier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the EDA object\n",
    "eda = EDA(Anomalies_OutliersAdjusted_df)\n",
    "\n",
    "# Plot the boxplot\n",
    "eda.plot_boxplot_max_depth('MaxDepth_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting Anomaly Depth: A Machine Learning Approach**\n",
    "This exercise aims to predict the maximum depth of anomalies for educational purposes. The applications of this prediction include filling in missing data and forecasting the future growth of anomalies, particularly the maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.callbacks import DeltaYStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame\n",
    "Anomalies_EngineeringFeatures_df = Anomalies_OutliersAdjusted_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Anomaly Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import Anomaly_mapping, plot_anomalies_by_year\n",
    "\n",
    "# Define the parameters\n",
    "increment_size = 1000\n",
    "relative_distance_threshold = 0.1  # meters\n",
    "orientation_threshold = 10  # degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Mapping Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Anomaly_mapping using AnomaliesProc_ML_Ready\n",
    "# anomaly_mapper = Anomaly_mapping(Anomalies_EngineeringFeatures_df, relative_distance_threshold, orientation_threshold)\n",
    "\n",
    "# # Call the process_in_increments method and store the result in Anomalies_EngineeringFeatures_Mapped_df\n",
    "# anomaly_mapper.process_in_increments(save_path, increment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalies_EngineeringFeatures_Mapped_df = anomaly_mapper.concat_mapped_dfs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "# Plot_Anomaly_mapped_df_file_path = (os.path.join(save_path, f'Plot_Mapped_Anomalies.csv'))\n",
    "\n",
    "# Anomalies_EngineeringFeatures_Mapped_df[[ # type: ignore\n",
    "#     'GirthWeldNumber',\n",
    "#     'InspectionYear',\n",
    "#     'RelativeDistance_m',\n",
    "#     'Tag',\n",
    "#     'SignificantPointOrientation_deg'\n",
    "# ]].to_parquet(Plot_Anomaly_mapped_df_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the anomalies by year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data locally\n",
    "# Anomalies_EngineeringFeatures_Mapped_df = pd.read_parquet(os.path.join(save_path, 'Anomalies_Mapped_First_1000_GirthWelds.parquet'))\n",
    "\n",
    "# Load the data from github url\n",
    "url = 'https://github.com/Farhad-Davaripour/AI_Applications_in_Pipeline_Engineering/raw/main/Dataset/processed_data/Anomalies_Mapped_First_1000_GirthWelds.parquet'\n",
    "\n",
    "# Use pandas with fsspec to read the Parquet file directly\n",
    "Anomalies_EngineeringFeatures_Mapped_df = pd.read_parquet(url, engine='pyarrow', storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the anomalies by year\n",
    "plot_anomalies_by_year(Anomalies_EngineeringFeatures_Mapped_df, 14, figsize=(8, 3)) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Aspect Ratio and Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import FeatureEngineering\n",
    "\n",
    "# Create an instance of the class with your dataframe\n",
    "feature_engineering = FeatureEngineering(Anomalies_EngineeringFeatures_Mapped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the aspect ratio\n",
    "Anomalies_EngineeringFeatures_Geometric_df = feature_engineering.compute_aspect_ratio()\n",
    "\n",
    "# Calculate the feature area\n",
    "Anomalies_EngineeringFeatures_Geometric_df = feature_engineering.calculate_feature_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns\n",
    "selected_columns = ['FeatureLength_mm', 'FeatureWidth_mm', 'AspectRatio', 'FeatureArea_mm2',]\n",
    "\n",
    "# Creating a new DataFrame with only the selected columns\n",
    "Anomalies_EngineeringFeatures_Geometric_df[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Radial to Cyclic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the angular features\n",
    "angle_columns = ['SignificantPointOrientation_deg']\n",
    "Anomalies_EngineeringFeatures_Radial2Cyclic_df = feature_engineering.add_angular_features(angle_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns that end with '_rad'\n",
    "rad_columns = [col for col in Anomalies_EngineeringFeatures_Radial2Cyclic_df.columns if col.startswith('SignificantPointOrientation')]\n",
    "# Creating a new DataFrame with only the selected columns\n",
    "Anomalies_EngineeringFeatures_Radial2Cyclic_df[rad_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Tag Erroneous Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import ErroneousAnomalyProcessor\n",
    "\n",
    "# Detect errors in mapped anomalies\n",
    "anomaly_processor = ErroneousAnomalyProcessor(Anomalies_EngineeringFeatures_Radial2Cyclic_df)\n",
    "\n",
    "# Apply the detect_errors method\n",
    "Anomalies_EngineeringFeatures_Tagging_df = Anomalies_EngineeringFeatures_Radial2Cyclic_df.copy()\n",
    "Anomalies_EngineeringFeatures_Tagging_df['ErrorClassification'] = Anomalies_EngineeringFeatures_Tagging_df.apply(\n",
    "    anomaly_processor.detect_errors, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_processor = ErroneousAnomalyProcessor(Anomalies_EngineeringFeatures_Tagging_df)\n",
    "\n",
    "# Print anomaly statistics\n",
    "anomaly_processor.print_error_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Include Second Prior Inspection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import add_dprev_features\n",
    "\n",
    "# Add the secont previous inspection year features to the DataFrame\n",
    "Old_Anomalies_EngineeringFeatures_Dprev_df = add_dprev_features(Anomalies_EngineeringFeatures_Tagging_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data locally\n",
    "# Old_Anomalies_EngineeringFeatures_Dprev_df = pd.read_parquet(os.path.join(save_path, 'Old_Anomalies_EngineeringFeatures_Dprev.parquet'))\n",
    "\n",
    "# Load from github url\n",
    "url = 'https://github.com/Farhad-Davaripour/AI_Applications_in_Pipeline_Engineering/raw/main/Dataset/processed_data/Old_Anomalies_EngineeringFeatures_Dprev.parquet'\n",
    "\n",
    "# Use pandas with fsspec to read the Parquet file directly\n",
    "Old_Anomalies_EngineeringFeatures_Dprev_df = pd.read_parquet(url, engine='pyarrow', storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Filter Anomalies\n",
    "This section should ideally be moved from Feature Engineering to Data Pre-processing step. However, the next step of including data from the second prior inspection is computationally intensive. The curation process in this step will reduce the data population, thereby decreasing computational latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the 'Okay' records\n",
    "Anomalies_EngineeringFeatures_Filtered_df = Old_Anomalies_EngineeringFeatures_Dprev_df[Old_Anomalies_EngineeringFeatures_Dprev_df.ErrorClassification == 'Okay']\n",
    "\n",
    "# Filter the DataFrame to include only the 'old' and 'new' records\n",
    "Old_Anomalies_EngineeringFeatures_Filtered_df = Anomalies_EngineeringFeatures_Filtered_df[Anomalies_EngineeringFeatures_Filtered_df.Tag == 'old']\n",
    "New_Anomalies_EngineeringFeatures_Filtered_df = Anomalies_EngineeringFeatures_Filtered_df[Anomalies_EngineeringFeatures_Filtered_df.Tag == 'new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Estimated Anomaly Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Old_Anomalies_EngineeringFeatures_EstGeometry_df = Old_Anomalies_EngineeringFeatures_Filtered_df.copy()\n",
    "\n",
    "Old_Anomalies_EngineeringFeatures_EstGeometry_df['Estimated_FeatureLength_mm'] = (\n",
    "    2 * Old_Anomalies_EngineeringFeatures_EstGeometry_df['Prev_FeatureLength_mm'] -\n",
    "    Old_Anomalies_EngineeringFeatures_EstGeometry_df['DPrev_FeatureLength_mm']\n",
    ")\n",
    "\n",
    "Old_Anomalies_EngineeringFeatures_EstGeometry_df['Estimated_FeatureWidth_mm'] = (\n",
    "    2 * Old_Anomalies_EngineeringFeatures_EstGeometry_df['Prev_FeatureWidth_mm'] -\n",
    "    Old_Anomalies_EngineeringFeatures_EstGeometry_df['DPrev_FeatureWidth_mm']\n",
    ")\n",
    "\n",
    "Old_Anomalies_EngineeringFeatures_EstGeometry_df['Powered_Prev_MaxDepth_mm'] = (\n",
    "    Old_Anomalies_EngineeringFeatures_EstGeometry_df['Prev_MaxDepth_mm'] ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Encoding Anomaly Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import AnomalyClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to be used for clustering\n",
    "clustering_features = [\n",
    "    'RelativeDistance_m',\n",
    "    'FeatureLength_mm',\n",
    "    'FeatureWidth_mm',\n",
    "    'MaxDepth_mm',\n",
    "    'SignificantPointOrientation_deg',\n",
    "    'Prev_RelativeDistance_m',\n",
    "    'Prev_FeatureLength_mm',\n",
    "    'Prev_FeatureWidth_mm',\n",
    "    'Prev_MaxDepth_mm',\n",
    "    'Prev_SignificantPointOrientation_deg',\n",
    "    'JointLength_m',\n",
    "    'SeamOrientation_deg',\n",
    "    'StartPointDistance_m',\n",
    "    'StartPointOrientation_deg',\n",
    "    'EndPointDistance_m',\n",
    "    'EndPointOrientation_deg',\n",
    "    'SignificantPointRelDistance_m',\n",
    "    'WallThickness_mm',\n",
    "    'AspectRatio',\n",
    "    'FeatureArea_mm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AnomalyClusterer class\n",
    "clusterer = AnomalyClusterer(Old_Anomalies_EngineeringFeatures_EstGeometry_df, clustering_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "Old_Anomalies_EngineeringFeatures_Clustered_df = clusterer.perform_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Old_Anomalies_EngineeringFeatures_Clustered_df[['GirthWeldNumber',\n",
    "                                                'InspectionYear',\n",
    "                                                'RelativeDistance_m',\n",
    "                                                'SignificantPointOrientation_deg',\n",
    "                                                'anomaly_type']\n",
    "                                                ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance ratio for each principal component\n",
    "clusterer.plot_pca_explained_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in a two-dimensional space using first two principal components\n",
    "clusterer.visualize_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regularization algorithm is used for to identify feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = Old_Anomalies_EngineeringFeatures_Clustered_df.drop(\n",
    "    columns=[\n",
    "        'MaxDepth_mm',\n",
    "        'Tag',\n",
    "        'ErrorClassification',\n",
    "        'DepthChange'\n",
    "    ]\n",
    ")\n",
    "\n",
    "target = Old_Anomalies_EngineeringFeatures_Clustered_df['MaxDepth_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import FeatureImportance\n",
    "\n",
    "feature_importance = FeatureImportance(features, target)\n",
    "\n",
    "# Perform the steps\n",
    "feature_importance.standardize_features()\n",
    "feature_importance.split_data()\n",
    "feature_importance.perform_grid_search()\n",
    "feature_importance.fit_best_lasso()\n",
    "feature_importance.calculate_coefficients()\n",
    "\n",
    "# Plot the coefficients\n",
    "feature_importance.plot_coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-zero coefficients\n",
    "feature_importance.plot_non_zero_coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the important features\n",
    "importance_df = feature_importance.importance_df\n",
    "important_features = importance_df[importance_df['Coefficient'].abs() > 0.01].Feature.tolist()\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Predicting Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Defining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Old_Anomalies_Training_df = Old_Anomalies_EngineeringFeatures_Clustered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target = Old_Anomalies_Training_df['MaxDepth_mm']\n",
    "\n",
    "# Define wall thickness variable\n",
    "wt_mm = Old_Anomalies_EngineeringFeatures_Clustered_df.WallThickness_mm\n",
    "\n",
    "# Keep all features, including rows where target is an outlier\n",
    "features = Old_Anomalies_Training_df[important_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Training Pipeline and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import TrainingPipeline\n",
    "\n",
    "# Create an instance of the TrainingPipeline class\n",
    "ML_pipeline = TrainingPipeline(features, target)\n",
    "print(\"Pipeline instance created\")\n",
    "\n",
    "# Scale the features\n",
    "ML_pipeline.scale_features()\n",
    "print(\"Features scaled\")\n",
    "\n",
    "# Split the data\n",
    "ML_pipeline.split_data(handle_imbalance=True)\n",
    "print(\"Data split\")\n",
    "\n",
    "# Perform hyperparameter tuning and return the best parameters\n",
    "best_params = ML_pipeline.hyperparameter_tuning()\n",
    "print(\"Best parameters found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fit the model using the best hyper parameters\n",
    "best_model = ML_pipeline.fit_model()\n",
    "print(\"Model fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and print the metrics across minority class\n",
    "ML_pipeline.evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Previous performance metrics\n",
    "{'RMSE': 0.2721, 'MAE': 0.224, 'R2': 0.9164, 'MAPE': 27.4117, 'ME': 0.0209}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Imbalance Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = DPrev_Old_Filtered_Anomaly_mapped_df['MaxDepth_mm'].value_counts().sort_index()\n",
    "sns.histplot(Old_Anomalies_Training_df['MaxDepth_mm'], bins=100)\n",
    "plt.title('Hist Plot of MaxDepth_mm')\n",
    "plt.xlabel('MaxDepth_mm')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results and evaluating prediction accuracy\n",
    "results = ML_pipeline.plot_prediction_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_scatter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Anomaly Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Training and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to remove outliers only from the target variable\n",
    "target = Old_Anomalies_Training_df['MaxDepth_mm']\n",
    "\n",
    "# Define the list of features\n",
    "feature_columns = [\n",
    "        'GirthWeldNumber',\n",
    "        'InspectionYear',\n",
    "        'RelativeDistance_m',\n",
    "        'Estimated_FeatureLength_mm',\n",
    "        'Estimated_FeatureWidth_mm',\n",
    "        'SignificantPointOrientation_deg',\n",
    "        'Prev_InspectionYear',\n",
    "        'Prev_RelativeDistance_m',\n",
    "        'Prev_FeatureLength_mm',\n",
    "        'Prev_FeatureWidth_mm',\n",
    "        'Prev_MaxDepth_mm',\n",
    "        'Powered_Prev_MaxDepth_mm',\n",
    "        'Prev_SignificantPointOrientation_deg',\n",
    "        'DPrev_RelativeDistance_m',\n",
    "        'DPrev_FeatureLength_mm',\n",
    "        'DPrev_FeatureWidth_mm',\n",
    "        'DPrev_MaxDepth_mm',\n",
    "        'DPrev_SignificantPointOrientation_deg',\n",
    "        'SignificantPointOrientation_deg_sin',\n",
    "        'SignificantPointOrientation_deg_cos'\n",
    "    ]\n",
    "\n",
    "# Keep all features, including rows where target is an outlier\n",
    "features = Old_Anomalies_Training_df[feature_columns]\n",
    "\n",
    "# Ensure features and target have the same index\n",
    "features = features.loc[target.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import TrainingPipeline\n",
    "\n",
    "# Create an instance of the TrainingPipeline class\n",
    "ML_pipeline = TrainingPipeline(features, target)\n",
    "print(\"Pipeline instance created\")\n",
    "\n",
    "# Scale the features\n",
    "ML_pipeline.scale_features()\n",
    "print(\"Features scaled\")\n",
    "\n",
    "# Split the data\n",
    "ML_pipeline.split_data(handle_imbalance=True)\n",
    "print(\"Data split\")\n",
    "\n",
    "# Perform hyperparameter tuning and return the best parameters\n",
    "best_params = ML_pipeline.hyperparameter_tuning()\n",
    "print(\"Best parameters found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fit the model using the best hyper parameters\n",
    "best_model = ML_pipeline.fit_model()\n",
    "print(\"Model fitted\")\n",
    "\n",
    "# Evaluate the model and print the metrics\n",
    "evaluation_metrics = ML_pipeline.evaluate_model()\n",
    "for metrics,performance in evaluation_metrics.items():\n",
    "    print(f\"{metrics}: {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Visualizing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ML_pipeline.plot_prediction_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_pipeline.plot_scatter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Predicting Future Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import AnomalyPredictionPipeline\n",
    "\n",
    "Old_Anomalies_Prediction_df = Old_Anomalies_Training_df.copy()\n",
    "Old_Anomalies_Prediction_df['WallThickness_mm'] = wt_mm\n",
    "\n",
    "prev_inspection_year = 7\n",
    "next_inspection_year = 9\n",
    "\n",
    "# Create an instance of the pipeline\n",
    "pipeline = AnomalyPredictionPipeline(model=best_model, df= Old_Anomalies_Prediction_df, prev_inspection_year=7, next_inspection_year=9)\n",
    "\n",
    "# Prepare data\n",
    "Old_Anomalies_Prediction_df = pipeline.prepare_data(wt_mm, feature_columns, target.name)\n",
    "\n",
    "# Make predictions\n",
    "Old_Anomalies_Prediction_df = pipeline.make_predictions( feature_columns)\n",
    "\n",
    "# Perform analytics\n",
    "pipeline.perform_analytics((6, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
